[package]
authors = ["WhytCard Team"]
description = "Local LLM inference engine for WhytCard using llama.cpp"
edition = "2021"
license = "GPL-3.0"
name = "whytcard-llm"
repository = "https://github.com/WhytcardAI/WhytCard"
version = "0.1.0"

[dependencies]
# llama.cpp Rust bindings
llama-cpp-2 = "0.1"

# Async runtime
tokio = { version = "1", features = ["full", "sync"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Error handling
thiserror = "1"

# Logging
tracing = "0.1"

# Utilities
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["serde", "v4"] }

# HuggingFace Hub for model downloads (optional)
hf-hub = { version = "0.3", optional = true }

[dev-dependencies]
tempfile = "3"
tokio-test = "0.4"

[features]
cuda = ["llama-cpp-2/cuda"]
default = []
metal = ["llama-cpp-2/metal"]
